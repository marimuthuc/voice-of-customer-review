{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice of Customer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim as gensim_vis\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the .csv file using Pandas. Take a look at the top few records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = pd.read_csv(\"dataset/K8 Reviews v0.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14675 entries, 0 to 14674\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  14675 non-null  int64 \n",
      " 1   review     14675 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 229.4+ KB\n"
     ]
    }
   ],
   "source": [
    "reviews_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good but need updates and improvements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Worst mobile i have bought ever, Battery is dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when I will get my 10% cash back.... its alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The worst phone everThey have changed the last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Only I'm telling don't buyI'm totally disappoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Phone is awesome. But while charging, it heats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>The battery level has worn down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>It's over hitting problems...and phone hanging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>A lot of glitches dont buy this thing better g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1             Good but need updates and improvements\n",
       "1          0  Worst mobile i have bought ever, Battery is dr...\n",
       "2          1  when I will get my 10% cash back.... its alrea...\n",
       "3          1                                               Good\n",
       "4          0  The worst phone everThey have changed the last...\n",
       "5          0  Only I'm telling don't buyI'm totally disappoi...\n",
       "6          1  Phone is awesome. But while charging, it heats...\n",
       "7          0                    The battery level has worn down\n",
       "8          0  It's over hitting problems...and phone hanging...\n",
       "9          0  A lot of glitches dont buy this thing better g..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Normalize casings for the review text and extract the text into a list for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some text pre-processing\n",
    "# Handler Functions for Text Preprocessing\n",
    "\n",
    "token = ToktokTokenizer()\n",
    "lemma = WordNetLemmatizer()\n",
    "# nltk.download(\"wordnet\")\n",
    "punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~0123456789'\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Clean Text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\r\", \" \", text)\n",
    "    text = re.sub(r\"<td>\", \" \", text)\n",
    "    text = re.sub(r\"</td>\", \" \", text)\n",
    "    text = re.sub(r\"<tr>\", \" \", text)\n",
    "    text = re.sub(r\"</tr>\", \" \", text)\n",
    "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                        u\"\\U00002702-\\U000027B0\"\n",
    "                        u\"\\U000024C2-\\U0001F251\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def strip_list_noempty(mylist):\n",
    "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
    "    return [item for item in newlist if item != '']\n",
    "\n",
    "def clean_punct(text): \n",
    "    words=token.tokenize(text)\n",
    "    punctuation_filtered = []\n",
    "    regex = re.compile('[%s]' % re.escape(punct))\n",
    "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
    "    for w in words:\n",
    "        punctuation_filtered.append(regex.sub('', w))   \n",
    "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
    "    return ' '.join(map(str, filtered_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Review Text to Lowercase and few more Text cleaning\n",
    "reviews_data.loc[:, 'review'] = reviews_data['review'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Emojis\n",
    "reviews_data.loc[:, 'review'] = reviews_data['review'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning Punctuations\n",
    "reviews_data.loc[:, 'review'] = reviews_data['review'].apply(lambda x: clean_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of Words in each review\n",
    "reviews_data['number_of_words'] = reviews_data.review.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>number_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>good but need updates and improvements</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>worst mobile i have bought ever battery is dra...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when i will get my cash back its already january</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>the worst phone everthey have changed the last...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review  \\\n",
       "0          1             good but need updates and improvements   \n",
       "1          0  worst mobile i have bought ever battery is dra...   \n",
       "2          1   when i will get my cash back its already january   \n",
       "3          1                                               good   \n",
       "4          0  the worst phone everthey have changed the last...   \n",
       "\n",
       "   number_of_words  \n",
       "0                6  \n",
       "1               88  \n",
       "2               10  \n",
       "3                1  \n",
       "4               27  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remiving Reviews with One and Two Words\n",
    "required_dataset = reviews_data[reviews_data['number_of_words']>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12118 entries, 0 to 14674\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   sentiment        12118 non-null  int64 \n",
      " 1   review           12118 non-null  object\n",
      " 2   number_of_words  12118 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 378.7+ KB\n"
     ]
    }
   ],
   "source": [
    "required_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Reviews to List\n",
    "reviews = required_dataset['review'].to_list()\n",
    "type(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good but need updates and improvements',\n",
       " 'worst mobile i have bought ever battery is draining like hell backup is only to hours with internet uses even if i put mobile idle its getting dischargedthis is biggest lie from amazon amp lenove which is not at all expected they are making full by saying that battery is mah amp booster charger is fake it takes at least to hours to be fully chargeddo not know how lenovo will survive by making full of usplease don t go for this else you will regret like me',\n",
       " 'when i will get my cash back its already january',\n",
       " 'the worst phone everthey have changed the last phone but the problem is still same and the amazon is not returning the phone highly disappointing of amazon',\n",
       " 'only i am telling do not buyi am totally disappointedpoor batterypoor camerawaste of money']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenize the reviews using NLTKs word_tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_tokens = [word_tokenize(item) for item in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good', 'but', 'need', 'updates', 'and', 'improvements'], ['worst', 'mobile', 'i', 'have', 'bought', 'ever', 'battery', 'is', 'draining', 'like', 'hell', 'backup', 'is', 'only', 'to', 'hours', 'with', 'internet', 'uses', 'even', 'if', 'i', 'put', 'mobile', 'idle', 'its', 'getting', 'dischargedthis', 'is', 'biggest', 'lie', 'from', 'amazon', 'amp', 'lenove', 'which', 'is', 'not', 'at', 'all', 'expected', 'they', 'are', 'making', 'full', 'by', 'saying', 'that', 'battery', 'is', 'mah', 'amp', 'booster', 'charger', 'is', 'fake', 'it', 'takes', 'at', 'least', 'to', 'hours', 'to', 'be', 'fully', 'chargeddo', 'not', 'know', 'how', 'lenovo', 'will', 'survive', 'by', 'making', 'full', 'of', 'usplease', 'don', 't', 'go', 'for', 'this', 'else', 'you', 'will', 'regret', 'like', 'me'], ['when', 'i', 'will', 'get', 'my', 'cash', 'back', 'its', 'already', 'january'], ['the', 'worst', 'phone', 'everthey', 'have', 'changed', 'the', 'last', 'phone', 'but', 'the', 'problem', 'is', 'still', 'same', 'and', 'the', 'amazon', 'is', 'not', 'returning', 'the', 'phone', 'highly', 'disappointing', 'of', 'amazon'], ['only', 'i', 'am', 'telling', 'do', 'not', 'buyi', 'am', 'totally', 'disappointedpoor', 'batterypoor', 'camerawaste', 'of', 'money']]\n"
     ]
    }
   ],
   "source": [
    "print(review_tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Perform parts-of-speech tagging on each sentence using the NLTK POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('good', 'JJ'), ('but', 'CC'), ('need', 'VBP'), ('updates', 'NNS'), ('and', 'CC'), ('improvements', 'NNS')], [('worst', 'JJS'), ('mobile', 'NN'), ('i', 'NN'), ('have', 'VBP'), ('bought', 'VBN'), ('ever', 'RB'), ('battery', 'NN'), ('is', 'VBZ'), ('draining', 'VBG'), ('like', 'IN'), ('hell', 'NN'), ('backup', 'NN'), ('is', 'VBZ'), ('only', 'RB'), ('to', 'TO'), ('hours', 'NNS'), ('with', 'IN'), ('internet', 'NN'), ('uses', 'NNS'), ('even', 'RB'), ('if', 'IN'), ('i', 'JJ'), ('put', 'VBP'), ('mobile', 'JJ'), ('idle', 'NN'), ('its', 'PRP$'), ('getting', 'VBG'), ('dischargedthis', 'NN'), ('is', 'VBZ'), ('biggest', 'JJS'), ('lie', 'NN'), ('from', 'IN'), ('amazon', 'NN'), ('amp', 'NN'), ('lenove', 'NN'), ('which', 'WDT'), ('is', 'VBZ'), ('not', 'RB'), ('at', 'IN'), ('all', 'DT'), ('expected', 'VBN'), ('they', 'PRP'), ('are', 'VBP'), ('making', 'VBG'), ('full', 'JJ'), ('by', 'IN'), ('saying', 'VBG'), ('that', 'DT'), ('battery', 'NN'), ('is', 'VBZ'), ('mah', 'JJ'), ('amp', 'JJ'), ('booster', 'NN'), ('charger', 'NN'), ('is', 'VBZ'), ('fake', 'JJ'), ('it', 'PRP'), ('takes', 'VBZ'), ('at', 'IN'), ('least', 'JJS'), ('to', 'TO'), ('hours', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('fully', 'RB'), ('chargeddo', 'JJ'), ('not', 'RB'), ('know', 'VB'), ('how', 'WRB'), ('lenovo', 'JJ'), ('will', 'MD'), ('survive', 'VB'), ('by', 'IN'), ('making', 'VBG'), ('full', 'JJ'), ('of', 'IN'), ('usplease', 'JJ'), ('don', 'NN'), ('t', 'NN'), ('go', 'VBP'), ('for', 'IN'), ('this', 'DT'), ('else', 'JJ'), ('you', 'PRP'), ('will', 'MD'), ('regret', 'VB'), ('like', 'IN'), ('me', 'PRP')]]\n"
     ]
    }
   ],
   "source": [
    "review_postags = [pos_tag(item) for item in review_tokens]\n",
    "print(review_postags[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. For the topic model, we should  want to include only nouns.\n",
    "    1. Find out all the POS tags that correspond to nouns.\n",
    "    2. Limit the data to only terms with these tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['updates', 'improvements'], ['mobile', 'i', 'battery', 'hell', 'backup', 'hours', 'internet', 'uses', 'idle', 'dischargedthis', 'lie', 'amazon', 'amp', 'lenove', 'battery', 'booster', 'charger', 'hours', 'don', 't'], ['i', 'cash'], ['phone', 'everthey', 'phone', 'problem', 'amazon', 'phone', 'amazon'], ['camerawaste', 'money']]\n"
     ]
    }
   ],
   "source": [
    "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "review_postags_nouns = []\n",
    "\n",
    "for item in review_postags:\n",
    "    noun_tokens = [token_tag[0] for token_tag in item if token_tag[1] in noun_tags]\n",
    "    review_postags_nouns.append(noun_tokens)\n",
    "    \n",
    "print(review_postags_nouns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Lemmatize. \n",
    "    1. Different forms of the terms need to be treated as one.\n",
    "    2. No need to provide POS tag to lemmatizer for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['update', 'improvement'], ['mobile', 'i', 'battery', 'hell', 'backup', 'hour', 'internet', 'us', 'idle', 'dischargedthis', 'lie', 'amazon', 'amp', 'lenove', 'battery', 'booster', 'charger', 'hour', 'don', 't'], ['i', 'cash'], ['phone', 'everthey', 'phone', 'problem', 'amazon', 'phone', 'amazon'], ['camerawaste', 'money']]\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "review_postags_nouns_lemmed = []\n",
    "\n",
    "for item in review_postags_nouns:\n",
    "    lemmed_tokens = [wnl.lemmatize(token, 'n') for token in item]\n",
    "    review_postags_nouns_lemmed.append(lemmed_tokens)\n",
    "    \n",
    "print(review_postags_nouns_lemmed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Remove stopwords and punctuation (if there are any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['update', 'improvement'], ['mobile', 'battery', 'hell', 'backup', 'hour', 'internet', 'us', 'idle', 'dischargedthis', 'lie', 'amazon', 'amp', 'lenove', 'battery', 'booster', 'charger', 'hour'], ['cash'], ['phone', 'everthey', 'phone', 'problem', 'amazon', 'phone', 'amazon'], ['camerawaste', 'money']]\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "punc = list(string.punctuation)\n",
    "\n",
    "custom_sw = sw + punc\n",
    "\n",
    "review_preprocessed = []\n",
    "\n",
    "for item in review_postags_nouns_lemmed:\n",
    "    if len(item)>0:        \n",
    "        preprocessed_tokens = [token for token in item if token not in custom_sw and len(token)>1]\n",
    "        review_preprocessed.append(preprocessed_tokens)\n",
    "    else:\n",
    "        review_preprocessed.append(item)\n",
    "    \n",
    "print(review_preprocessed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Create a topic model using LDA on the cleaned up data with 12 topics.\n",
    "    1. Print out the top terms for each topic.\n",
    "    2. What is the coherence of the model with the c_v metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.corpora.dictionary.Dictionary"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary(review_preprocessed)\n",
    "type(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<1118 unique tokens: ['improvement', 'update', 'amazon', 'amp', 'backup']...>\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=.8 ,keep_n=None)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_text = [dictionary.doc2bow(item) for item in review_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1)],\n",
       " [(2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 2),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 2),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1)],\n",
       " [(15, 1)],\n",
       " [(2, 2), (16, 3), (17, 1)],\n",
       " [(18, 1)]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'battery' comes 1 times in this sample review\n",
      "Word 'phone' comes 1 times in this sample review\n",
      "Word 'lenovo' comes 1 times in this sample review\n",
      "Word 'product' comes 2 times in this sample review\n",
      "Word 'camera' comes 1 times in this sample review\n"
     ]
    }
   ],
   "source": [
    "sample_bow = bow_text[20]\n",
    "\n",
    "for item in sample_bow:\n",
    "    print(\"Word '{}' comes {} times in this sample review\".format(dictionary[item[0]], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model using Gensim\n",
    "lda_model = gensim.models.LdaMulticore(bow_text, \n",
    "                                   num_topics = 12, \n",
    "                                   id2word = dictionary,\n",
    "                                   random_state=1,                                    \n",
    "                                   passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0 \n",
      "Words: 0.328*\"mobile\" + 0.054*\"hai\" + 0.039*\"box\" + 0.020*\"earphone\" + 0.018*\"ho\" + 0.016*\"hi\" + 0.016*\"bill\" + 0.014*\"note\" + 0.014*\"lenovo\" + 0.014*\"headset\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.311*\"phone\" + 0.052*\"call\" + 0.025*\"lenovo\" + 0.025*\"note\" + 0.025*\"option\" + 0.021*\"issue\" + 0.017*\"budget\" + 0.015*\"app\" + 0.015*\"feature\" + 0.013*\"apps\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.104*\"issue\" + 0.050*\"network\" + 0.046*\"device\" + 0.035*\"update\" + 0.034*\"sim\" + 0.030*\"use\" + 0.028*\"time\" + 0.025*\"day\" + 0.023*\"phone\" + 0.019*\"lenovo\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.188*\"money\" + 0.082*\"waste\" + 0.073*\"value\" + 0.067*\"phone\" + 0.055*\"handset\" + 0.048*\"month\" + 0.043*\"product\" + 0.029*\"superb\" + 0.027*\"worth\" + 0.024*\"item\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.081*\"charger\" + 0.060*\"speaker\" + 0.048*\"phone\" + 0.046*\"note\" + 0.033*\"turbo\" + 0.022*\"screen\" + 0.020*\"time\" + 0.020*\"lenovo\" + 0.019*\"hour\" + 0.019*\"power\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.228*\"camera\" + 0.080*\"phone\" + 0.041*\"mode\" + 0.033*\"battery\" + 0.029*\"quality\" + 0.024*\"depth\" + 0.020*\"front\" + 0.016*\"clarity\" + 0.015*\"effect\" + 0.015*\"mp\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.176*\"quality\" + 0.110*\"camera\" + 0.105*\"phone\" + 0.048*\"sound\" + 0.026*\"system\" + 0.022*\"headphone\" + 0.020*\"look\" + 0.018*\"software\" + 0.017*\"mi\" + 0.015*\"lenovo\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.265*\"problem\" + 0.099*\"feature\" + 0.085*\"heating\" + 0.068*\"phone\" + 0.061*\"screen\" + 0.033*\"camera\" + 0.032*\"cast\" + 0.016*\"option\" + 0.014*\"tv\" + 0.013*\"network\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.068*\"performance\" + 0.065*\"note\" + 0.060*\"camera\" + 0.036*\"processor\" + 0.034*\"phone\" + 0.032*\"display\" + 0.031*\"glass\" + 0.030*\"ram\" + 0.029*\"gb\" + 0.025*\"everything\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.354*\"battery\" + 0.087*\"backup\" + 0.043*\"life\" + 0.039*\"hour\" + 0.037*\"performance\" + 0.036*\"camera\" + 0.036*\"phone\" + 0.032*\"issue\" + 0.031*\"drain\" + 0.029*\"heat\"\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.208*\"phone\" + 0.181*\"price\" + 0.069*\"range\" + 0.035*\"charge\" + 0.035*\"time\" + 0.028*\"feature\" + 0.017*\"android\" + 0.015*\"application\" + 0.014*\"stock\" + 0.014*\"review\"\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.203*\"product\" + 0.060*\"amazon\" + 0.056*\"service\" + 0.055*\"phone\" + 0.031*\"lenovo\" + 0.031*\"day\" + 0.025*\"time\" + 0.024*\"delivery\" + 0.021*\"customer\" + 0.019*\"issue\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"\\nTopic: {} \\nWords: {}\".format(idx, topic ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for this LDA model is:  0.5949711742750197\n"
     ]
    }
   ],
   "source": [
    "# calculating Coherence Score\n",
    "coherence_score = CoherenceModel(model=lda_model, texts=review_preprocessed, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_score.get_coherence()\n",
    "print('Coherence Score for this LDA model is: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Analyze the topics through the business lens.\n",
    "    1. Determine which of the topics can be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Topic Models\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_text, dictionary)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'LDA_model_vis'+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the LDA Visualization, there is a high overlap between several topics. Hence the following topics can be combined together\n",
    "\n",
    "1. Topics 3,4,7,9, 10\n",
    "2. Topics 2,5,6,8\n",
    "3. Topics 1\n",
    "4. Topic 12\n",
    "5. Topic 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Create topic model using LDA with what you think is the optimal number of topics\n",
    "    1. What is the coherence of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0 \n",
      "Words: 0.174*\"camera\" + 0.091*\"quality\" + 0.074*\"mobile\" + 0.024*\"feature\" + 0.024*\"money\" + 0.017*\"hai\" + 0.017*\"value\" + 0.017*\"performance\" + 0.016*\"price\" + 0.013*\"display\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.293*\"phone\" + 0.036*\"price\" + 0.030*\"note\" + 0.025*\"feature\" + 0.021*\"lenovo\" + 0.015*\"range\" + 0.013*\"screen\" + 0.012*\"glass\" + 0.011*\"camera\" + 0.011*\"service\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.064*\"issue\" + 0.032*\"network\" + 0.031*\"time\" + 0.030*\"problem\" + 0.028*\"device\" + 0.024*\"mobile\" + 0.023*\"lenovo\" + 0.021*\"update\" + 0.020*\"call\" + 0.019*\"note\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.181*\"product\" + 0.093*\"problem\" + 0.041*\"amazon\" + 0.038*\"heating\" + 0.032*\"money\" + 0.026*\"service\" + 0.023*\"issue\" + 0.023*\"month\" + 0.023*\"waste\" + 0.021*\"battery\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.141*\"battery\" + 0.057*\"phone\" + 0.056*\"camera\" + 0.036*\"backup\" + 0.027*\"performance\" + 0.025*\"charger\" + 0.024*\"hour\" + 0.020*\"day\" + 0.018*\"life\" + 0.017*\"speaker\"\n"
     ]
    }
   ],
   "source": [
    "lda_model1 = gensim.models.LdaMulticore(bow_text, \n",
    "                                   num_topics = 5, \n",
    "                                   id2word = dictionary,  \n",
    "                                   random_state=1,\n",
    "                                   passes = 50)\n",
    "\n",
    "for idx, topic in lda_model1.print_topics(-1):\n",
    "    print(\"\\nTopic: {} \\nWords: {}\".format(idx, topic ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for new LDA model_1 is:  0.632227583634284\n"
     ]
    }
   ],
   "source": [
    "coherence_score1 = CoherenceModel(model=lda_model1, texts=review_preprocessed, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda1 = coherence_score1.get_coherence()\n",
    "print('Coherence Score for new LDA model_1 is: ', coherence_lda1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Topic Number 5 is giving better coherence score compared to Topic number 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. The business should  be able to interpret the topics.\n",
    "    1. Name each of the identified topics.\n",
    "    2. Create a table with the topic name and the top 10 terms in each to present to the  business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following Topics could be inferred from the LDA model\n",
    "\n",
    "* Topic 1: Camera Related\n",
    "* Topic 2: Pricing Related\n",
    "* Topic 3: Network and Call Related\n",
    "* Topic 4: Product Issues like Service, Heating\n",
    "* Topic 5: Battery Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Word_1</th>\n",
       "      <td>camera</td>\n",
       "      <td>phone</td>\n",
       "      <td>issue</td>\n",
       "      <td>product</td>\n",
       "      <td>battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_2</th>\n",
       "      <td>quality</td>\n",
       "      <td>price</td>\n",
       "      <td>network</td>\n",
       "      <td>problem</td>\n",
       "      <td>phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_3</th>\n",
       "      <td>mobile</td>\n",
       "      <td>note</td>\n",
       "      <td>time</td>\n",
       "      <td>amazon</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_4</th>\n",
       "      <td>feature</td>\n",
       "      <td>feature</td>\n",
       "      <td>problem</td>\n",
       "      <td>heating</td>\n",
       "      <td>backup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_5</th>\n",
       "      <td>money</td>\n",
       "      <td>lenovo</td>\n",
       "      <td>device</td>\n",
       "      <td>money</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_6</th>\n",
       "      <td>hai</td>\n",
       "      <td>range</td>\n",
       "      <td>mobile</td>\n",
       "      <td>service</td>\n",
       "      <td>charger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_7</th>\n",
       "      <td>value</td>\n",
       "      <td>screen</td>\n",
       "      <td>lenovo</td>\n",
       "      <td>issue</td>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_8</th>\n",
       "      <td>performance</td>\n",
       "      <td>glass</td>\n",
       "      <td>update</td>\n",
       "      <td>month</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_9</th>\n",
       "      <td>price</td>\n",
       "      <td>camera</td>\n",
       "      <td>call</td>\n",
       "      <td>waste</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_10</th>\n",
       "      <td>display</td>\n",
       "      <td>service</td>\n",
       "      <td>note</td>\n",
       "      <td>battery</td>\n",
       "      <td>speaker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Topic_1  Topic_2  Topic_3  Topic_4      Topic_5\n",
       "Word_1        camera    phone    issue  product      battery\n",
       "Word_2       quality    price  network  problem        phone\n",
       "Word_3        mobile     note     time   amazon       camera\n",
       "Word_4       feature  feature  problem  heating       backup\n",
       "Word_5         money   lenovo   device    money  performance\n",
       "Word_6           hai    range   mobile  service      charger\n",
       "Word_7         value   screen   lenovo    issue         hour\n",
       "Word_8   performance    glass   update    month          day\n",
       "Word_9         price   camera     call    waste         life\n",
       "Word_10      display  service     note  battery      speaker"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words = {}\n",
    "\n",
    "for idx, topic in lda_model1.print_topics(-1): \n",
    "    temp = []\n",
    "    for item in topic.split('+'):\n",
    "        item_alpha = [letter for letter in item if letter.isalpha()]\n",
    "        temp.append(\"\".join(item_alpha))    \n",
    "    topic_words[('Topic_'+str(idx+1))] = temp\n",
    "\n",
    "topic_table = pd.DataFrame(topic_words)    \n",
    "topic_table.index = ['Word_'+str(i+1) for i in range(topic_table.shape[0])]\n",
    "topic_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared1 = pyLDAvis.gensim.prepare(lda_model1, bow_text, dictionary)\n",
    "pyLDAvis.save_html(LDAvis_prepared1, 'LDA_model_topic_5'+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
